# The Role of the "System Prompt" in Large Language Models (LLM)
### Introduction: 
The system prompt is a fundamental element in the operation of modern LLMs.
It is an initial context message, invisible to the end user, that defines the basic
behavior of the model.
## 1. What is a system prompt and what impact does it have on the model's behavior?
A system prompt is a predefined directive provided to the model beforeany user input. Essentially, it's an initial message that establishes the AI's role, its goals, the tone, and the rules it should follow during the conversation.[1] This system prompt overrides what the user asks: it serves as a "high-level instruction" that the model must always keep in mind, even if the user provides conflicting input.[2]
For example, a system prompt might say, "You are ChatGPT, a large language model trained by OpenAI, and you must respect
these rules..."—thus defining the AI ​​assistant's behavior from the start.
### Impact on the behaviour
The content of the system prompt directly influences the style and limits of the model's responses. Because it is processed before any user message, the model is "directed" to respond in line with these basic instructions. Studies have shown that the system prompt can define key behaviors of the model, such as requiring it to always provide explanations or to apply filters and warnings on certain topics. In other words, the system prompt acts as a sort of "default personality" and set of rules: for example, if it includes the instruction to reject inappropriate requests, the model will tend to decline such requests; if it requires providing disclaimers on medical topics, the LLM will add those disclaimers to health-related responses.
This mechanism is used by providers to ensure consistency and safety in responses. In short, **the system prompt guides** the model and has a dominant impact on behavior: it takes precedence over user requests and ensures that the assistant follows certain criteria and maintains a consistent style. This can also introduce side effects—for example, bias—if the system instructions
contain preconceptions: the location (system vs. user) of a piece of information can significantly alter
the model's response.[2]
## 2. Differences in system prompt handling in ChatGPT,API GPT and Ollama
Different LLM platforms handle the system prompt in slightly different ways:
1. **ChatGPT (official OpenAI web interface)**: ChatGPT internally uses a fixed system prompt, set by OpenAI, which defines the model's default behavior (for example: "You are ChatGPT, a large language model trained by OpenAI, ..." with guidelines on the style and allowed content). This system prompt is neither visible nor directly modifiable by the user in the regular interface. Until mid-2023, users could not influence the system prompt; later, OpenAI introduced **Custom Instructions**, allowing users to add persistent contextual information for all conversations. Once activated, **these custom instructions are incorporated as part of the initial prompt** for every new chat (effectively added to the system prompt). In other words, ChatGPT remains bound to a basic system prompt (containing, for example, the phrase "You are a helpful assistant" and various safety rules), but the user can add additional permanent instructions through Custom Instructions. These user instructions are also treated as part of the system context, influencing all responses without revealing the full system prompt imposed by OpenAI to the user.
2. **API OpenAI(GPT)**:  Through the API, it is **the developer user** who sets the system prompt, so access/modification is total: you just need to include a message with the role *system* in the call payload. Each API call is isolated, which means that if you want to maintain the same system prompt across multiple turns, you need to resend it each time. There is no other uncontrollable system prompt layer: what is sent as a system message is exactly what the model will see as the initial instruction. It is worth noting that some cloud platforms based on OpenAI (e.g., Azure OpenAI) provide additional predefined system prompts or supplementary guidelines: for example, Microsoft Azure suggests that deployers always include a system prompt that instructs the model not to reveal its confidential prompts. In general, however, when using the OpenAI API directly, modifying the system prompt is entirely at the user's discretion: you can change the text, tone, rules as you like to adapt the LLM to the desired application.[2]
3. **Ollama**: In Ollama, the user has **full control** over a local model's system prompt. As seen, it is possible to edit it in the model's Modelfile (which is freely accessible and modifiable by the user). This means that anyone using Ollama can see what the current system prompt of a model is (*ollama show --modelfile* displays the Modelfile and therefore the SYSTEM section, if present) and can modify it by creating a new custom model with a different system prompt. Additionally, via API/SDK, Ollama allows passing system prompt strings for each interaction without having to recreate a model from scratch. Essentially, the platform is designed to make it easy for developers both to access the system instructions associated with a model and to replace or update them. There are no limitations imposed by third parties, as it is a local/open source environment: the only potential limitation is the prompt structure required by the model.[5] This means that it is possible to 'pre-load' a default personality or behavior at the model level.
In Ollama, **the system prompt is editable and configurable by the user**: it can be persisted at the model level (by setting it once in the Modelfile) or passed dynamically for each conversation. This flexibility is similar to that of the OpenAI API, with the difference that Ollama automatically manages the template details based on the model (transparently for the user) and natively supports the separation between system context and user prompt.

## Access and editability of the SystemPrompt on Ollama:
In Ollama, **the user has full control over the system prompt of a local model**. As mentioned, it is possible to edit it in the model's Modelfile (which is freely accessible and modifiable by the user). This means that anyone using Ollama can see what the current system prompt of a model is (*ollama show --modelfile* displays the Modelfile and thus the SYSTEM section, if present) and can modify it by creating a new custom model with a different system prompt. Additionally, via API/SDK, Ollama allows system prompt strings to be passed in each interaction without needing to recreate a model from scratch. Essentially, the platform is designed to make it easy for developers both to access the system instructions associated with a model and to replace or update them. There are no limitations imposed by third parties, as it is a local/open-source environment: the only potential constraint is the prompt structure required by the model.
[5]

## Conclusion

The prompt system represents one of the most powerful—and at the same time most delicate—tools for controlling the behavior of Large Language Models. It does not add new knowledge to the dominant model, but it guides its behavior, the priority of instructions, and the way information is used to generate responses. Its privileged position in the context hierarchy means that the model tends to respect it even when this conflicts with subsequent user requests.
In contexts where the prompt system is fully controllable, as in the case of Ollama, it becomes possible to directly experience the impact of such instructions on the model's behavior and accuracy. Setting a prompt system that requires, for example, "always answer incorrectly" does not change the model's internal capabilities, but it does alter its operational objective: the model will continue to "reason correctly" internally, but will deliberately attempt to produce incorrect responses to comply with the higher-level instruction. This demonstrates that the prompt system neither intrinsically improves nor degrades the model's intelligence; rather, it overrides the optimality criterion by which responses are selected.
The literature shows that this mechanism has significant consequences: the instructions provided in the prompt system carry a disproportionate weight compared to those included in user messages and can introduce systematic biases, reduce current accuracy, or force undesirable behavior, even when the user attempts to correct them. This makes the prompt system a powerful tool for security and consistency, but also a potential source of structural errors if improperly designed.
In conclusion, the ability offered by Ollama to directly access and modify the prompt system is extremely useful from an experimental and educational perspective: it allows for transparent observation of how the "system level" influences the model's behavior. However, this freedom also highlights a fundamental principle: the system controls behavior, not the truth. A conscious and minimal use of such instructions is therefore essential to preserving the accuracy and reliability of language models. [2]



[1] Why Long System Prompts Hurt Context Windows (and How to Fix It) | byLucas Valbuena | Data Science Collective | Medium https://medium.com/data-science-collectivewhy-long-system-prompts-hurt-context-windows-and-how-to-fix-it-7a3696e1cdf9

[2]Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs) https://arxiv.org/html/2505.21091v2#S1 

[3] When “A Helpful Assistant” Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models https://arxiv.org/html/2311.10054v3#S5 

[4]ChatGPT Custom Instructions | OpenAI Help Center

[5]Ollama https://docs.ollama.com/modelfile
